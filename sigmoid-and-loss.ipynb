{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, transformer_encoder, embedding_dim, num_classes, fc_hidden_dims=[512, 256, 128], dropout=0.1):\n",
    "        \"\"\"\n",
    "        멀티레이블 분류를 위한 클래스\n",
    "        Args:\n",
    "            transformer_encoder: 사전 구현된 트랜스포머 인코더 모델\n",
    "            embedding_dim: 인코더의 출력 임베딩 차원\n",
    "            num_classes: 분류할 클래스 개수\n",
    "            fc_hidden_dims: 3개 완전 연결 층의 은닉층 차원 리스트\n",
    "            dropout: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        \n",
    "        # 트랜스포머 인코더\n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        \n",
    "        # 분류 헤드 (3개의 완전 연결 층)\n",
    "        self.classifier = nn.Sequential(\n",
    "            # 첫 번째 완전 연결 층\n",
    "            nn.Linear(embedding_dim, fc_hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # 두 번째 완전 연결 층\n",
    "            nn.Linear(fc_hidden_dims[0], fc_hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # 세 번째 완전 연결 층\n",
    "            nn.Linear(fc_hidden_dims[1], fc_hidden_dims[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            # 출력 층 (시그모이드 활성화 함수)\n",
    "            nn.Linear(fc_hidden_dims[2], num_classes),\n",
    "            nn.Sigmoid()  # 멀티레이블 분류를 위한 시그모이드 활성화 함수\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        모델 순전파\n",
    "        Args:\n",
    "            x: 입력 텐서 (batch_size, seq_length)\n",
    "            mask: 어텐션 마스크 (optional)\n",
    "        Returns:\n",
    "            각 클래스에 대한 확률 (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # 트랜스포머 인코더 통과\n",
    "        encoder_output = self.transformer_encoder(x, mask)  # (batch_size, seq_length, embedding_dim)\n",
    "        \n",
    "        # 시퀀스의 [CLS] 토큰(첫 번째 토큰) 사용 또는 전체 시퀀스의 평균 사용\n",
    "        # 방법 1: [CLS] 토큰 표현 사용\n",
    "        sequence_representation = encoder_output[:, 0, :]  # (batch_size, embedding_dim)\n",
    "        \n",
    "        # 방법 2: 전체 시퀀스의 평균 사용 (선택적)\n",
    "        # sequence_representation = torch.mean(encoder_output, dim=1)  # (batch_size, embedding_dim)\n",
    "        \n",
    "        # 분류 헤드 통과\n",
    "        logits = self.classifier(sequence_representation)  # (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class WeightedBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        가중치가 적용된 바이너리 크로스 엔트로피 손실 함수\n",
    "        Args:\n",
    "            pos_weight: 각 클래스의 양성 샘플에 대한 가중치 (클래스별 가중치 텐서)\n",
    "            reduction: 손실 감소 방식 ('mean', 'sum', 'none')\n",
    "        \"\"\"\n",
    "        super(WeightedBinaryCrossEntropyLoss, self).__init__()\n",
    "        self.pos_weight = pos_weight\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        손실 계산\n",
    "        Args:\n",
    "            logits: 모델 출력 (batch_size, num_classes)\n",
    "            targets: 타겟 레이블 (batch_size, num_classes), 이진값 (0 또는 1)\n",
    "        Returns:\n",
    "            손실 값\n",
    "        \"\"\"\n",
    "        # 이미 시그모이드가 적용된 출력이므로 BCELoss 사용\n",
    "        if self.pos_weight is not None:\n",
    "            # 클래스별 가중치 적용\n",
    "            weight = self.pos_weight.expand_as(targets)\n",
    "            \n",
    "            # 양성(1) 및 음성(0) 샘플에 대한 가중치 적용\n",
    "            weight_pos = weight * targets\n",
    "            weight_neg = (1 - targets)\n",
    "            weights = weight_pos + weight_neg\n",
    "            \n",
    "            # 가중치가 적용된 이진 크로스 엔트로피 손실\n",
    "            loss = F.binary_cross_entropy(logits, targets, weight=weights, reduction='none')\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy(logits, targets, reduction='none')\n",
    "        \n",
    "        # 감소 방식에 따라 손실 처리\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:  # 'none'\n",
    "            return loss\n",
    "\n",
    "def calculate_class_weights(labels, beta=0.999):\n",
    "    \"\"\"\n",
    "    클래스 불균형을 다루기 위한 가중치 계산 \n",
    "    (Cui et al., 2019) \"Class-balanced loss based on effective number of samples\"\n",
    "    \n",
    "    Args:\n",
    "        labels: 훈련 데이터의 레이블 텐서 (num_samples, num_classes)\n",
    "        beta: 클래스 불균형 조정을 위한 스무딩 팩터\n",
    "    Returns:\n",
    "        클래스별 가중치 텐서\n",
    "    \"\"\"\n",
    "    # 각 클래스의 샘플 수 계산\n",
    "    num_samples_per_class = torch.sum(labels, dim=0)  # (num_classes)\n",
    "    \n",
    "    # 0으로 나누기 방지\n",
    "    num_samples_per_class = torch.clamp(num_samples_per_class, min=1)\n",
    "    \n",
    "    # 효과적인 샘플 수 계산\n",
    "    effective_num = 1.0 - torch.pow(beta, num_samples_per_class)\n",
    "    effective_num = torch.clamp(effective_num, min=1e-8)\n",
    "    \n",
    "    # 클래스 가중치 계산\n",
    "    weights = (1.0 - beta) / effective_num\n",
    "    \n",
    "    # 가중치 정규화 (합이 클래스 수가 되도록)\n",
    "    weights = weights / torch.sum(weights) * len(num_samples_per_class)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# 모델 학습 함수\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    한 에폭 동안 모델 학습\n",
    "    Args:\n",
    "        model: 학습할 모델\n",
    "        dataloader: 데이터 로더\n",
    "        criterion: 손실 함수\n",
    "        optimizer: 최적화 알고리즘\n",
    "        device: 학습 장치 ('cpu' 또는 'cuda')\n",
    "    Returns:\n",
    "        평균 손실\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # 데이터를 지정된 장치로 이동\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 순전파\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate(model, dataloader, criterion, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    모델 평가\n",
    "    Args:\n",
    "        model: 평가할 모델\n",
    "        dataloader: 데이터 로더\n",
    "        criterion: 손실 함수\n",
    "        device: 평가 장치\n",
    "        threshold: 분류 임계값\n",
    "    Returns:\n",
    "        평균 손실, 정밀도, 재현율, F1 점수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # 순전파\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 임계값 적용하여 예측 결과 변환 (0.5 이상이면 1, 미만이면 0)\n",
    "            preds = (outputs >= threshold).float()\n",
    "            \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "    \n",
    "    # 예측값과 타겟값 결합\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    \n",
    "    # 평가 지표 계산\n",
    "    # 각 클래스별 TP, FP, FN 계산\n",
    "    tp = torch.sum((all_preds == 1) & (all_targets == 1), dim=0).float()\n",
    "    fp = torch.sum((all_preds == 1) & (all_targets == 0), dim=0).float()\n",
    "    fn = torch.sum((all_preds == 0) & (all_targets == 1), dim=0).float()\n",
    "    \n",
    "    # 정밀도, 재현율, F1 점수 계산\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    \n",
    "    # 매크로 평균 (모든 클래스에 대한 평균)\n",
    "    macro_precision = precision.mean().item()\n",
    "    macro_recall = recall.mean().item()\n",
    "    macro_f1 = f1.mean().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), macro_precision, macro_recall, macro_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
