{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        멀티헤드 어텐션 모듈\n",
    "        Args:\n",
    "            embedding_dim: 임베딩 차원\n",
    "            num_heads: 어텐션 헤드 개수\n",
    "            dropout: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert embedding_dim % num_heads == 0, \"임베딩 차원은 헤드 개수로 나누어 떨어져야 합니다.\"\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        \n",
    "        # 쿼리, 키, 밸류 가중치 행렬\n",
    "        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # 출력 가중치 행렬\n",
    "        self.out_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        멀티헤드 어텐션 순전파\n",
    "        Args:\n",
    "            query: 쿼리 텐서 (batch_size, seq_length, embedding_dim)\n",
    "            key: 키 텐서 (batch_size, seq_length, embedding_dim)\n",
    "            value: 밸류 텐서 (batch_size, seq_length, embedding_dim)\n",
    "            mask: 어텐션 마스크 (optional)\n",
    "        Returns:\n",
    "            output: 어텐션 결과 (batch_size, seq_length, embedding_dim)\n",
    "            attention_weights: 어텐션 가중치\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 선형 투영 및 헤드로 분할\n",
    "        q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch, num_heads, seq_len_q, head_dim)\n",
    "        k = self.k_linear(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch, num_heads, seq_len_k, head_dim)\n",
    "        v = self.v_linear(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch, num_heads, seq_len_v, head_dim)\n",
    "        \n",
    "        # 스케일드 닷-프로덕트 어텐션\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale  # (batch, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # 마스크 적용 (필요한 경우)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 어텐션 가중치 계산\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 가중치와 밸류의 곱\n",
    "        attention_output = torch.matmul(attention_weights, v)  # (batch, num_heads, seq_len_q, head_dim)\n",
    "        \n",
    "        # 헤드 결합 및 출력 투영\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.embedding_dim)  # (batch, seq_len_q, embedding_dim)\n",
    "        output = self.out_linear(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim, ff_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        피드포워드 네트워크 모듈\n",
    "        Args:\n",
    "            embedding_dim: 임베딩 차원\n",
    "            ff_dim: 피드포워드 네트워크 내부 차원\n",
    "            dropout: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(embedding_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        피드포워드 네트워크 순전파\n",
    "        Args:\n",
    "            x: 입력 텐서 (batch_size, seq_length, embedding_dim)\n",
    "        Returns:\n",
    "            출력 텐서 (batch_size, seq_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        # 첫 번째 선형 변환 및 활성화 함수 (ReLU)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 두 번째 선형 변환\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        \"\"\"\n",
    "        트랜스포머 인코더 레이어\n",
    "        Args:\n",
    "            embedding_dim: 임베딩 차원\n",
    "            num_heads: 어텐션 헤드 개수\n",
    "            ff_dim: 피드포워드 네트워크 내부 차원\n",
    "            dropout: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(embedding_dim, ff_dim, dropout)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        인코더 레이어 순전파\n",
    "        Args:\n",
    "            x: 입력 텐서 (batch_size, seq_length, embedding_dim)\n",
    "            mask: 어텐션 마스크 (optional)\n",
    "        Returns:\n",
    "            출력 텐서 (batch_size, seq_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        # 멀티헤드 셀프 어텐션 (서브레이어 1)\n",
    "        attn_output, _ = self.self_attention(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # 잔차 연결 및 레이어 정규화\n",
    "        \n",
    "        # 피드포워드 네트워크 (서브레이어 2)\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        ff_output = self.dropout2(ff_output)\n",
    "        out2 = self.layernorm2(out1 + ff_output)  # 잔차 연결 및 레이어 정규화\n",
    "        \n",
    "        return out2\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers=2, num_heads=8, ff_dim=2048, max_seq_length=100, dropout=0.1):\n",
    "        \"\"\"\n",
    "        트랜스포머 인코더 모델\n",
    "        Args:\n",
    "            vocab_size: 어휘 크기 (성분 개수)\n",
    "            embedding_dim: 임베딩 차원\n",
    "            num_layers: 인코더 레이어 개수\n",
    "            num_heads: 어텐션 헤드 개수\n",
    "            ff_dim: 피드포워드 네트워크 내부 차원\n",
    "            max_seq_length: 최대 시퀀스 길이\n",
    "            dropout: 드롭아웃 비율\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        # 임베딩 레이어\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 포지셔널 인코딩\n",
    "        self.pos_encoding = self._create_positional_encoding(max_seq_length, embedding_dim)\n",
    "        \n",
    "        # 인코더 레이어 스택\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embedding_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def _create_positional_encoding(self, seq_length, embedding_dim):\n",
    "        \"\"\"\n",
    "        포지셔널 인코딩 생성\n",
    "        Args:\n",
    "            seq_length: 시퀀스 길이\n",
    "            embedding_dim: 임베딩 차원\n",
    "        Returns:\n",
    "            포지셔널 인코딩 텐서\n",
    "        \"\"\"\n",
    "        pos_encoding = np.zeros((seq_length, embedding_dim))\n",
    "        positions = np.arange(seq_length)[:, np.newaxis]\n",
    "        dimensions = np.arange(embedding_dim)[np.newaxis, :]\n",
    "        \n",
    "        angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / embedding_dim)\n",
    "        angle_rads = positions * angle_rates\n",
    "        \n",
    "        pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        return torch.FloatTensor(pos_encoding).unsqueeze(0)  # (1, seq_length, embedding_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        트랜스포머 인코더 순전파\n",
    "        Args:\n",
    "            x: 입력 텐서 (batch_size, seq_length)\n",
    "            mask: 어텐션 마스크 (optional)\n",
    "        Returns:\n",
    "            출력 텐서 (batch_size, seq_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # 임베딩 및 포지셔널 인코딩 적용\n",
    "        x = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        x = x + self.pos_encoding[:, :seq_length, :].to(x.device)  # 포지셔널 인코딩 추가\n",
    "        x = self.dropout(x)\n",
    "        x = self.layernorm(x)  # 초기 정규화\n",
    "        \n",
    "        # 인코더 레이어 통과\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, encoder, embedding_dim, num_labels=15):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = nn.Linear(embedding_dim, num_labels)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # 인코더의 출력 가져오기\n",
    "        encoder_output = self.encoder(x, mask)\n",
    "        \n",
    "        # [CLS] 토큰 또는 평균 풀링 사용\n",
    "        # 옵션 1: 첫 번째 토큰 ([CLS]) 사용\n",
    "        # pooled_output = encoder_output[:, 0]\n",
    "        \n",
    "        # 옵션 2: 평균 풀링\n",
    "        pooled_output = torch.mean(encoder_output, dim=1)\n",
    "        \n",
    "        # 분류기에 통과\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
