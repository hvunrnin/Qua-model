{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['a', 'b', 'c', 'd', 'e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_id(corpus):\n",
    "    word_to_id = {}\n",
    "    for idx, word in enumerate(corpus):\n",
    "        word_to_id[word] = idx\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# word_to_id 함수는 이미 구현되어 있음\n",
    "def word_to_id(corpus):\n",
    "    word_to_id = {}\n",
    "    for idx, word in enumerate(corpus):\n",
    "        word_to_id[word] = idx\n",
    "    return word_to_id\n",
    "\n",
    "class IngredientEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        화장품 성분 임베딩 클래스\n",
    "        Args:\n",
    "            vocab_size: 어휘 크기 (성분 개수)\n",
    "            embedding_dim: 임베딩 차원\n",
    "        \"\"\"\n",
    "        super(IngredientEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        입력 텐서를 임베딩 벡터로 변환\n",
    "        Args:\n",
    "            x: 입력 텐서 (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            임베딩 벡터 (batch_size, sequence_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "def positional_encoding(seq_length, embedding_dim):\n",
    "    \"\"\"\n",
    "    Positional encoding 생성\n",
    "    Args:\n",
    "        seq_length: 시퀀스 길이\n",
    "        embedding_dim: 임베딩 차원\n",
    "    Returns:\n",
    "        positional encoding (seq_length, embedding_dim)\n",
    "    \"\"\"\n",
    "    # positional encoding matrix 초기화\n",
    "    pos_encoding = np.zeros((seq_length, embedding_dim))\n",
    "    \n",
    "    # position과 dimension 변수\n",
    "    positions = np.arange(seq_length)[:, np.newaxis]\n",
    "    dimensions = np.arange(embedding_dim)[np.newaxis, :]\n",
    "    \n",
    "    # 짝수 인덱스에는 sine 함수 적용, 홀수 인덱스에는 cosine 함수 적용\n",
    "    angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / embedding_dim)\n",
    "    angle_rads = positions * angle_rates\n",
    "    \n",
    "    # 짝수 인덱스에 sine 적용\n",
    "    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    \n",
    "    # 홀수 인덱스에 cosine 적용\n",
    "    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "class IngredientEmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_length):\n",
    "        \"\"\"\n",
    "        화장품 성분 임베딩과 positional encoding을 결합한 클래스\n",
    "        Args:\n",
    "            vocab_size: 어휘 크기 (성분 개수)\n",
    "            embedding_dim: 임베딩 차원\n",
    "            max_seq_length: 최대 시퀀스 길이\n",
    "        \"\"\"\n",
    "        super(IngredientEmbeddingWithPositionalEncoding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "        # positional encoding 생성\n",
    "        self.pos_encoding = positional_encoding(max_seq_length, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        입력 텐서에 대해 임베딩과 positional encoding을 적용\n",
    "        Args:\n",
    "            x: 입력 텐서 (batch_size, sequence_length)\n",
    "        Returns:\n",
    "            임베딩 + positional encoding 벡터 (batch_size, sequence_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        # 임베딩 계산\n",
    "        embeddings = self.embedding(x)\n",
    "        \n",
    "        # 시퀀스 길이 확인\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # positional encoding 적용 (시퀀스 길이에 맞게 자름)\n",
    "        positional_encodings = self.pos_encoding[:seq_length, :].unsqueeze(0)\n",
    "        \n",
    "        # 임베딩과 positional encoding 결합\n",
    "        return embeddings + positional_encodings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
